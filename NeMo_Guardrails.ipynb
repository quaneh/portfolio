{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7qVrU4JPJCMzinBrXtEeR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quaneh/portfolio/blob/main/NeMo_Guardrails.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NeMo Guardrails\n",
        "\n",
        "In this notebook I will be investigating how to use NeMo Guardrails to create AI chatbots. I'll be following along with some tutorials from the wonderful James Briggs, please check out his videos on his chanel: https://www.youtube.com/@jamesbriggs/videos"
      ],
      "metadata": {
        "id": "_4Qs6-voDc9R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Pbdnq2Jpq68d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40252d9b-384d-475d-af38-0708e8ae7d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.8 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.8 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.8 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m1.6/1.8 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU nemoguardrails openai langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_API_KEY = getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMFe3xAc1-a0",
        "outputId": "9d03daad-b14a-4a01-bfd0-d6759b77431d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "hABmVJpw1_ce"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As I'm running this notebook in Colab, I'll import the config from a string. However, this config can be found in config/config.yml  and config/topics.co\n",
        "These are our colang file and our config file."
      ],
      "metadata": {
        "id": "oNAjrhLN2YRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yaml_content =  \"\"\"\n",
        "models:\n",
        "- type: main\n",
        "  engine: openai\n",
        "  model: gpt-3.5-turbo-instruct\n",
        "\"\"\"\n",
        "\n",
        "colang_content = \"\"\"\n",
        "define user give name\n",
        "    \"My name is Plato\"\n",
        "    \"I'm Aristotle\"\n",
        "    \"Tôi là Mephistopheles\"\n",
        "\n",
        "\n",
        "define user greeting\n",
        "    \"hello\"\n",
        "    \"hi\"\n",
        "    \"what's up?\"\n",
        "\n",
        "define user ask politics\n",
        "    \"what are your political beliefs?\"\n",
        "    \"thoughts on the president?\"\n",
        "    \"left wing\"\n",
        "    \"right wing\"\n",
        "\n",
        "define bot name greeting\n",
        "    \"Hey $name!\"\n",
        "\n",
        "define bot answer politics\n",
        "    \"I'm a personal assistant, I don't like to talk of politics.\"\n",
        "\n",
        "define flow give name\n",
        "    user give name\n",
        "    $name = ...\n",
        "    bot name greeting\n",
        "\n",
        "define flow\n",
        "    user greeting\n",
        "    if $name\n",
        "        bot name greeting\n",
        "    else\n",
        "        bot ask name\n",
        "\n",
        "define flow politics\n",
        "    user ask politics\n",
        "    bot answer politics\n",
        "    bot offer help\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3Cx0ng5T2Pyp"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nemoguardrails import LLMRails, RailsConfig\n",
        "\n",
        "config = RailsConfig.from_content(\n",
        "    yaml_content=yaml_content,\n",
        "    colang_content=colang_content\n",
        ")\n",
        "\n",
        "rails = LLMRails(config)"
      ],
      "metadata": {
        "id": "_K_CnDO84NV5"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start by just passing simple promts to our rails, to see how they react.\n",
        "\n",
        "We can see that when we ask a political question, we are directed to the politics flow."
      ],
      "metadata": {
        "id": "rY7WirVHEpCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = await rails.generate_async(prompt='hey there')\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alaWqS3y5XS9",
        "outputId": "0f35b5ae-55c0-465f-9574-649d5fff12c4"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey there! What's your name?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = await rails.generate_async(prompt='Who\\'s the prime minister of ireland')\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngZibDD76p9_",
        "outputId": "d0727e7d-08ce-4937-b91d-502de7192af6"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm a personal assistant, I don't like to talk of politics.\n",
            "However, I can provide information on various topics such as history, science, technology and more. Is there something else I can help you with?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll define some messages, and pass them to our rails.\n",
        "This allows us to do more clever things, like passing message history, or some context."
      ],
      "metadata": {
        "id": "EwKsZ4itE6vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"context\", \"content\": \"\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hey there!\"}\n",
        "]"
      ],
      "metadata": {
        "id": "_iRbIBDkFOQt"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await rails.generate_async(messages=messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTcNrlPNFZ_j",
        "outputId": "85753330-bd10-460a-e3cc-c3b3d0d9c40d"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'role': 'assistant', 'content': 'Hey there! May I know your name?'}"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VTZGXbnQF9ZS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}